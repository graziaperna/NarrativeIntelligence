{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graziaperna/NarrativeIntelligence/blob/mt5/Correttoseq2seq_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RLNriItzzHgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ad7253-2ba3-42fc-f104-c9bf75789830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipeline\n",
            "  Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: pipeline\n",
            "Successfully installed pipeline-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezUXbmQAoP1v",
        "outputId": "35bfdbed-b955-4fae-cc95-71d679f19343"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gsarti/it5.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t4U3DLmoRv-",
        "outputId": "1f7c6146-d129-4881-9831-b3c0ebd7a74b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'it5'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 139 (delta 49), reused 96 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (139/139), 10.94 MiB | 12.50 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r it5/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrQMEH8-oT4U",
        "outputId": "66bc5413-35b2-49cf-8523-bd9deb3c530b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 1)) (4.41.2)\n",
            "Collecting datasets (from -r it5/requirements.txt (line 2))\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score (from -r it5/requirements.txt (line 3))\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score (from -r it5/requirements.txt (line 4))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 6)) (2.3.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 8)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r it5/requirements.txt (line 9)) (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (0.23.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r it5/requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r it5/requirements.txt (line 2)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r it5/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from transformers[sentencepiece]->-r it5/requirements.txt (line 1))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->-r it5/requirements.txt (line 2))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r it5/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r it5/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r it5/requirements.txt (line 2)) (3.9.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score->-r it5/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r it5/requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r it5/requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r it5/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r it5/requirements.txt (line 6)) (12.5.40)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r it5/requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r it5/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r it5/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r it5/requirements.txt (line 9)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r it5/requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r it5/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]->-r it5/requirements.txt (line 1)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r it5/requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score->-r it5/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r it5/requirements.txt (line 6)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=c6c815d37237ee9dfa857a02d8cdb1ae28d6a82537b79b3dd0b8dda9bcd34e40\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: xxhash, requests, dill, rouge-score, multiprocess, datasets, bert-score\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bert-score-0.3.13 datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 rouge-score-0.1.2 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!shuf datasetT5.jsonl > datasetT5_shuf.jsonl\n",
        "!cat datasetT5_shuf.jsonl | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCgg7gZLo9Ql",
        "outputId": "dbe74bd5-d74c-4bd4-ee3f-42541f4bdaeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 6500 datasetT5_shuf.jsonl > train.json\n",
        "!tail -n 650 datasetT5_shuf.jsonl > temp.json\n",
        "!head -n 500 temp.json > test.json\n",
        "!tail -n 150 temp.json > val.json\n",
        "!rm temp.json\n",
        "!cat train.json | wc -l\n",
        "!cat test.json | wc -l\n",
        "!cat val.json | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AptuSbfspmfq",
        "outputId": "f2c6e4c2-e5c7-44e0-839a-e8f8b2c82f80"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6500\n",
            "500\n",
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python it5/finetuning/run_seq2seq.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKiO7byyuF5p",
        "outputId": "e9046628-0399-45b7-8d19-c5f5089c6cf6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 18:25:16.245183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-06 18:25:16.245233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-06 18:25:16.247050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-06 18:25:16.258020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 18:25:17.697589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: run_seq2seq.py [-h] --model_name_or_path MODEL_NAME_OR_PATH [--config_name CONFIG_NAME]\n",
            "                      [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                      [--use_fast_tokenizer [USE_FAST_TOKENIZER]] [--no_use_fast_tokenizer]\n",
            "                      [--model_revision MODEL_REVISION] [--use_auth_token USE_AUTH_TOKEN]\n",
            "                      [--resize_position_embeddings RESIZE_POSITION_EMBEDDINGS]\n",
            "                      [--from_flax [FROM_FLAX]] [--dataset_name DATASET_NAME]\n",
            "                      [--dataset_config_name DATASET_CONFIG_NAME] [--source_column SOURCE_COLUMN]\n",
            "                      [--target_column TARGET_COLUMN] [--train_file TRAIN_FILE]\n",
            "                      [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]\n",
            "                      [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                      [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
            "                      [--max_source_length MAX_SOURCE_LENGTH]\n",
            "                      [--max_target_length MAX_TARGET_LENGTH]\n",
            "                      [--val_max_target_length VAL_MAX_TARGET_LENGTH]\n",
            "                      [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "                      [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                      [--max_eval_samples MAX_EVAL_SAMPLES]\n",
            "                      [--max_predict_samples MAX_PREDICT_SAMPLES] [--num_beams NUM_BEAMS]\n",
            "                      [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n",
            "                      [--no_ignore_pad_token_for_loss] [--source_prefix SOURCE_PREFIX]\n",
            "                      --output_dir OUTPUT_DIR [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                      [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]] [--do_predict [DO_PREDICT]]\n",
            "                      [--eval_strategy {no,steps,epoch}]\n",
            "                      [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                      [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                      [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                      [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                      [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                      [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                      [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                      [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]\n",
            "                      [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                      [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                      [--max_grad_norm MAX_GRAD_NORM] [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                      [--max_steps MAX_STEPS]\n",
            "                      [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]\n",
            "                      [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS] [--warmup_ratio WARMUP_RATIO]\n",
            "                      [--warmup_steps WARMUP_STEPS]\n",
            "                      [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
            "                      [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
            "                      [--log_on_each_node [LOG_ON_EACH_NODE]] [--no_log_on_each_node]\n",
            "                      [--logging_dir LOGGING_DIR] [--logging_strategy {no,steps,epoch}]\n",
            "                      [--logging_first_step [LOGGING_FIRST_STEP]] [--logging_steps LOGGING_STEPS]\n",
            "                      [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
            "                      [--no_logging_nan_inf_filter] [--save_strategy {no,steps,epoch}]\n",
            "                      [--save_steps SAVE_STEPS] [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                      [--save_safetensors [SAVE_SAFETENSORS]] [--no_save_safetensors]\n",
            "                      [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "                      [--save_only_model [SAVE_ONLY_MODEL]]\n",
            "                      [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
            "                      [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
            "                      [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED] [--data_seed DATA_SEED]\n",
            "                      [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
            "                      [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                      [--half_precision_backend {auto,apex,cpu_amp}]\n",
            "                      [--bf16_full_eval [BF16_FULL_EVAL]] [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                      [--tf32 TF32] [--local_rank LOCAL_RANK]\n",
            "                      [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl}]\n",
            "                      [--tpu_num_cores TPU_NUM_CORES] [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
            "                      [--debug DEBUG [DEBUG ...]] [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                      [--eval_steps EVAL_STEPS] [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                      [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
            "                      [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                      [--disable_tqdm DISABLE_TQDM]\n",
            "                      [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                      [--no_remove_unused_columns] [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                      [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                      [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                      [--greater_is_better GREATER_IS_BETTER]\n",
            "                      [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]\n",
            "                      [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS] [--fsdp_config FSDP_CONFIG]\n",
            "                      [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
            "                      [--accelerator_config ACCELERATOR_CONFIG] [--deepspeed DEEPSPEED]\n",
            "                      [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                      [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise}]\n",
            "                      [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]\n",
            "                      [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                      [--length_column_name LENGTH_COLUMN_NAME] [--report_to REPORT_TO]\n",
            "                      [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                      [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
            "                      [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
            "                      [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                      [--no_dataloader_pin_memory]\n",
            "                      [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
            "                      [--skip_memory_metrics [SKIP_MEMORY_METRICS]] [--no_skip_memory_metrics]\n",
            "                      [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "                      [--push_to_hub [PUSH_TO_HUB]]\n",
            "                      [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                      [--hub_model_id HUB_MODEL_ID]\n",
            "                      [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
            "                      [--hub_token HUB_TOKEN] [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
            "                      [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
            "                      [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
            "                      [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
            "                      [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
            "                      [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
            "                      [--no_eval_do_concat_batches] [--fp16_backend {auto,apex,cpu_amp}]\n",
            "                      [--evaluation_strategy {no,steps,epoch}]\n",
            "                      [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "                      [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "                      [--push_to_hub_token PUSH_TO_HUB_TOKEN] [--mp_parameters MP_PARAMETERS]\n",
            "                      [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
            "                      [--full_determinism [FULL_DETERMINISM]] [--torchdynamo TORCHDYNAMO]\n",
            "                      [--ray_scope RAY_SCOPE] [--ddp_timeout DDP_TIMEOUT]\n",
            "                      [--torch_compile [TORCH_COMPILE]]\n",
            "                      [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
            "                      [--torch_compile_mode TORCH_COMPILE_MODE]\n",
            "                      [--dispatch_batches DISPATCH_BATCHES] [--split_batches SPLIT_BATCHES]\n",
            "                      [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
            "                      [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
            "                      [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
            "                      [--optim_target_modules OPTIM_TARGET_MODULES]\n",
            "                      [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
            "                      [--sortish_sampler [SORTISH_SAMPLER]]\n",
            "                      [--predict_with_generate [PREDICT_WITH_GENERATE]]\n",
            "                      [--generation_max_length GENERATION_MAX_LENGTH]\n",
            "                      [--generation_num_beams GENERATION_NUM_BEAMS]\n",
            "                      [--generation_config GENERATION_CONFIG]\n",
            "run_seq2seq.py: error: the following arguments are required: --model_name_or_path, --output_dir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python it5/finetuning/run_seq2seq.py --model_name_or_path gsarti/it5-base --tokenizer_name gsarti/it5-base --do_train true --do_eval true  --do_predict true --num_train_epochs 5 --train_file train.json --validation_file val.json --test_file test.json --source_column source --target_column target --max_source_length 512 --max_target_length 256  --output_dir model_base_512_256_5/ --overwrite_output_dir true --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --save_strategy epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygz6q3r7ormL",
        "outputId": "ffae84ef-6129-4adb-a807-25a850ea9fb9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 18:25:24.366508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-06 18:25:24.366565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-06 18:25:24.367934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-06 18:25:24.375456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 18:25:25.468761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "06/06/2024 18:25:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "Generating train split: 6500 examples [00:00, 216649.66 examples/s]\n",
            "Generating valid split: 150 examples [00:00, 90485.49 examples/s]\n",
            "Generating test split: 500 examples [00:00, 220104.11 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 696/696 [00:00<00:00, 3.54MB/s]\n",
            "tokenizer_config.json: 100% 1.91k/1.91k [00:00<00:00, 12.6MB/s]\n",
            "tokenizer.json: 100% 1.01M/1.01M [00:00<00:00, 1.27MB/s]\n",
            "special_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 11.8MB/s]\n",
            "pytorch_model.bin: 100% 990M/990M [00:52<00:00, 19.0MB/s]\n",
            "Running tokenizer on train dataset:   0% 0/6500 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Running tokenizer on train dataset: 100% 6500/6500 [00:02<00:00, 2908.04 examples/s]\n",
            "Running tokenizer on validation dataset: 100% 150/150 [00:00<00:00, 2516.01 examples/s]\n",
            "Running tokenizer on prediction dataset: 100% 500/500 [00:00<00:00, 2753.49 examples/s]\n",
            "/content/it5/finetuning/run_seq2seq.py:517: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"rouge\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 5.65kB [00:00, 14.7MB/s]       \n",
            "{'loss': 1.9092, 'grad_norm': 62.23170471191406, 'learning_rate': 4.692307692307693e-05, 'epoch': 0.31}\n",
            "{'loss': 0.867, 'grad_norm': 31.13020133972168, 'learning_rate': 4.384615384615385e-05, 'epoch': 0.62}\n",
            "{'loss': 0.6517, 'grad_norm': 23.490772247314453, 'learning_rate': 4.0769230769230773e-05, 'epoch': 0.92}\n",
            "{'loss': 0.5632, 'grad_norm': 18.7443790435791, 'learning_rate': 3.769230769230769e-05, 'epoch': 1.23}\n",
            "{'loss': 0.5113, 'grad_norm': 26.23373031616211, 'learning_rate': 3.461538461538462e-05, 'epoch': 1.54}\n",
            "{'loss': 0.4868, 'grad_norm': 40.68355941772461, 'learning_rate': 3.153846153846154e-05, 'epoch': 1.85}\n",
            "{'loss': 0.4527, 'grad_norm': 23.54079818725586, 'learning_rate': 2.846153846153846e-05, 'epoch': 2.15}\n",
            "{'loss': 0.3964, 'grad_norm': 23.16007423400879, 'learning_rate': 2.5384615384615383e-05, 'epoch': 2.46}\n",
            "{'loss': 0.3801, 'grad_norm': 13.273028373718262, 'learning_rate': 2.230769230769231e-05, 'epoch': 2.77}\n",
            "{'loss': 0.41, 'grad_norm': 16.81606101989746, 'learning_rate': 1.923076923076923e-05, 'epoch': 3.08}\n",
            "{'loss': 0.3502, 'grad_norm': 14.995074272155762, 'learning_rate': 1.6153846153846154e-05, 'epoch': 3.38}\n",
            "{'loss': 0.351, 'grad_norm': 4.472652912139893, 'learning_rate': 1.3076923076923078e-05, 'epoch': 3.69}\n",
            "{'loss': 0.37, 'grad_norm': 14.997241973876953, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
            "{'loss': 0.3521, 'grad_norm': 17.851333618164062, 'learning_rate': 6.923076923076923e-06, 'epoch': 4.31}\n",
            "{'loss': 0.3314, 'grad_norm': 15.02600383758545, 'learning_rate': 3.846153846153847e-06, 'epoch': 4.62}\n",
            "{'loss': 0.3127, 'grad_norm': 37.138973236083984, 'learning_rate': 7.692307692307694e-07, 'epoch': 4.92}\n",
            "{'train_runtime': 2493.85, 'train_samples_per_second': 13.032, 'train_steps_per_second': 3.258, 'train_loss': 0.5407958618164063, 'epoch': 5.0}\n",
            "100% 8125/8125 [41:33<00:00,  3.26it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  total_flos               =  3356273GF\n",
            "  train_loss               =     0.5408\n",
            "  train_runtime            = 0:41:33.84\n",
            "  train_samples            =       6500\n",
            "  train_samples_per_second =     13.032\n",
            "  train_steps_per_second   =      3.258\n",
            "100% 38/38 [00:01<00:00, 20.71it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_loss               =     0.2043\n",
            "  eval_runtime            = 0:00:01.89\n",
            "  eval_samples            =        150\n",
            "  eval_samples_per_second =     79.064\n",
            "  eval_steps_per_second   =      20.03\n",
            " 74% 93/125 [00:12<00:08,  3.84it/s]Traceback (most recent call last):\n",
            "  File \"/content/it5/finetuning/run_seq2seq.py\", line 647, in <module>\n",
            "    main()\n",
            "  File \"/content/it5/finetuning/run_seq2seq.py\", line 602, in main\n",
            "    predict_results = trainer.predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 244, in predict\n",
            "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3648, in predict\n",
            "    output = eval_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3779, in evaluation_loop\n",
            "    all_preds.add(logits)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 326, in add\n",
            "    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 138, in nested_concat\n",
            "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 138, in <genexpr>\n",
            "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 140, in nested_concat\n",
            "    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 105, in torch_pad_and_concatenate\n",
            "    result = tensor1.new_full(new_shape, padding_index)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.81 GiB. GPU \n",
            " 74% 93/125 [00:13<00:04,  7.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_small\n",
        "!cp model_small_512_256_5/*.* model_small/"
      ],
      "metadata": {
        "id": "5fulnBFN_pUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabbf73e-f7f0-44fc-8c09-6c56ffaab5ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòmodel_small‚Äô: File exists\n",
            "cp: cannot stat 'model_small_512_256_5/*.*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_base\n",
        "!cp model_base_512_256_5/*.* model_base/"
      ],
      "metadata": {
        "id": "tr35TwdjDVmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23932598-21e1-45c0-fe92-eb601f0c8674"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòmodel_base‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcf model_small.tar.gz model_small/"
      ],
      "metadata": {
        "id": "76QWa-XQDeyY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcf model_base.tar.gz model_base/"
      ],
      "metadata": {
        "id": "Yc2BDYv2DldZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY-DjjKIGDF_",
        "outputId": "bcb44b6e-75ba-4a58-aaf7-417343055a96"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model_small.tar.gz '/content/drive/My Drive/temp/'\n"
      ],
      "metadata": {
        "id": "w3fk3zSsGOIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cea29f-8a7c-445d-d090-580d8a1bb60d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/drive/My Drive/temp/': Not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model_base.tar.gz '/content/drive/My Drive/temp/'\n"
      ],
      "metadata": {
        "id": "WIm5jj7_GVoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befc222e-e404-4eb8-eb0e-851b16313f43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/gdrive/My Drive/temp/': Not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "v2bPwedKq5f4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/model_small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/model_small\")\n",
        "\n",
        "ner_pipeline = pipeline(task=\"text2text-generation\",model=model,tokenizer=tokenizer , max_new_tokens=256)\n",
        "\n",
        "text = \"La giornata spunt√≤ serena e limpida per gli sposi che dopo aver riposato una notte a Como continuarono il loro viaggio verso la Tremezzina\"\n",
        "ner_results = ner_pipeline(text)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "KJzCVm-Mqsfl",
        "outputId": "7c323885-7165-49ad-b023-7b93437adc46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/content/model_small does not appear to have a file named config.json. Checkout 'https://huggingface.co//content/model_small/tree/None' for available files.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-661b581a16be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/model_small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/model_small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mner_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text2text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    838\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0;34mf\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0;34mf\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/model_small does not appear to have a file named config.json. Checkout 'https://huggingface.co//content/model_small/tree/None' for available files."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/model_base_512_256_5\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/model_base_512_256_5\")\n",
        "\n",
        "ner_pipeline = pipeline(task=\"text2text-generation\",model=model,tokenizer=tokenizer, max_new_tokens=256 )\n",
        "\n",
        "text = \"La giornata spunt√≤ serena e limpida per gli sposi che dopo aver riposato una notte a Como continuarono il loro viaggio verso la Tremezzina\"\n",
        "ner_results = ner_pipeline(text)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIrDeucozYde",
        "outputId": "e50ac8ab-d95e-443e-c014-4d2b234bf5ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'sposi [per] como [gpe] tremezzina [gpe]'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifica il percorso del file che vuoi salvare su Google Drive\n",
        "file_path = '/content/model_base_512_256_5/'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Copia il file su Google Drive\n",
        "!cp -r {file_path} {drive_path}"
      ],
      "metadata": {
        "id": "wHYPhUwEHqPU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifica il percorso del file che vuoi salvare su Google Drive\n",
        "file_path = '/content/model_base.tar.gz/'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Copia il file su Google Drive\n",
        "!cp {file_path} {drive_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0co8kMPI2L9",
        "outputId": "179f8aa7-ca87-40c7-ba03-b75b907224da"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/model_base.tar.gz/': Not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifica il percorso del file che vuoi salvare su Google Drive\n",
        "file_path = '/content/model_small.tar.gz/'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Copia il file su Google Drive\n",
        "!cp {file_path} {drive_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_nVxqCvI93c",
        "outputId": "829941ef-df1a-4d1e-e384-02a7535fee4a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/model_small.tar.gz/': Not a directory\n"
          ]
        }
      ]
    }
  ]
}